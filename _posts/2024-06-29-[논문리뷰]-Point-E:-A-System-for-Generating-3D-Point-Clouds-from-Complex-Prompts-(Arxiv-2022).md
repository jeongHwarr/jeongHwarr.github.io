---
layout: post
date: 2024-06-29
title: "[논문리뷰] Point-E: A System for Generating 3D Point Clouds from Complex Prompts (Arxiv 2022)"
tags: [Point E, Generative AI, 3D generation, 3D point cloud generation, text-to-3D, ]
categories: [Review, ]
---


오늘 소개드릴 논문은 OpenAI에서 발표했었던 텍스트를 이용해서 포인트 클라우드를 생성하여 3D를 생성하는 논문인[ point-E](https://arxiv.org/abs/2212.08751) 입니다. 코드는 해당 [링크](https://github.com/openai/point-e)에 공유 되어 있습니다. 먼저 핵심 요약은 아래와 같습니다. 


# 핵심 요약

- 관련 태스크: text-to-3D, point cloud generation
- 본 논문의 접근 방식
	- Text-to-image 모델과 image-to-3D 모델을 결합하여 두 방법의 장점을 합쳤다.
		- Text-to-image: 대규모 데이터가 존재하여 복잡하고 다양한 텍스트 프롬프트에 대해 적용 가능하다.
		- Image-to-3D: 소규모의 이미지 데이터와 3D 데이터 쌍에 대해 학습하여 3D 포인트 클라우드를 생성한다.
	- 세 단계의 프로세스로 구성됨
		- Text → 합성 뷰: GLIDE 모델
		- 합성 뷰 → 저해상도 포인트 클라우드: 트랜스포머 기반의 디퓨전 모델
		- 저해상도 포인트 클라우드 → 고해상도 포인트 클라우드: 트랜스포머 기반의 디퓨전 모델
- 결과
	- 샘플 품질 측면에서 SOTA 성능이 아니지만, 샘플링 속도가 1~2배 더 빠르다.
	- 텍스트 프롬프트에 의해 조건화된 다양하고 복잡한 3D 형상을 효율적으로 생성할 수 있다.

# Abstract


최근 텍스트 조건부 3D 객체 생성 기술이 놀라운 발전을 이루며 주목받고 있습니다. 이러한 기술은 사용자가 입력한 텍스트를 바탕으로 3D 모델을 생성해주기 때문에 다양한 분야에서 잠재력을 인정받고 있습니다. 하지만 현재의 최신 모델(State-of-the-Art, SOTA)은 하나의 3D 객체를 만들기 위해 다수의 GPU 자원이 요구되며, 이를 처리하는 데 상당한 시간이 소요된다는 한계가 있습니다.


이러한 문제를 해결하고자, 본 논문에서는 단일 GPU에서 1~2분이라는 짧은 시간 내에 3D 모델을 생성할 수 있는 새로운 방법을 제시했습니다. 연구진은 텍스트-이미지 디퓨전 모델을 활용하여 먼저 단일 합성 이미지를 생성한 후, 이를 바탕으로 두 번째 디퓨전 모델을 사용해 3D 포인트 클라우드를 만들어냅니다. 이 접근법은 기존의 복잡한 과정에 비해 훨씬 간소화되었으며, 더 적은 자원으로도 3D 객체를 신속하게 생성할 수 있습니다.


물론, 생성된 샘플의 품질은 현재 최고 수준의 모델과 비교할 때 SOTA 성능은 아니라고 합니다. 그러나 속도 면에서는 기존 방법보다 1~2배 이상 빠른 성과를 보였으며, 이러한 효율성은 특히 자원이 제한된 환경에서 큰 강점이 될 것입니다.


빠른 3D 생성이 필요한 다양한 산업에서 본 연구가 제시하는 방법은 앞으로 더 많은 가능성을 열어줄 것으로 기대됩니다.


# 1. Introduction


**Text-to-image 생성 모델에서 text-to-vide/3D로의 발전**


최근 텍스트에서 이미지를 생성하는 text-to-image 모델이 빠르게 발전하면서, 몇 초 만에 자연어를 고품질의 이미지로 변환하는 기술이 가능해졌습니다. 이러한 놀라운 성과에 자극받아, 최근 연구들은 비디오나 3D 객체와 같은 다른 도메인에서도 텍스트 조건부 생성을 탐구하고 있습니다. 이번 논문 역시 text-to-3D 생성 문제에 중점을 두고, 더 빠르고 효율적인 3D 생성 방법을 제안합니다.


**최근 text-to-3D 합성의 분류**


최근의 text-to-3D 합성 기술은 크게 두 가지 카테고리로 나눌 수 있습니다.

1. **쌍을 이룬(paired) 데이터 또는 레이블이 없는 3D 데이터를 활용한 직접 학습 방식**

	이 방식은 텍스트와 3D 데이터 간의 쌍을 이룬 데이터셋 또는 레이블이 없는 3D 데이터를 사용하여 생성 모델을 직접 학습시키는 방법입니다.

	- **장점**: 기존 생성 모델링 방식을 사용하여 효율적으로 샘플을 생성할 수 있습니다.
	- **단점**: 대규모 3D 데이터셋의 부재로 인해 다양한 텍스트 프롬프트에 대한 확장이 어렵습니다. 데이터셋의 한계와 확장성의 어려움이 존재합니다.
2. **사전 학습된 Text-to-Image 모델을 활용한 3D 표현법 최적화 방식**

	이 방식은 미리 학습된 text-to-image 모델을 활용해 미분가능한 3D 표현법을 최적화하는 방법입니다.

	- **장점**: 복잡하고 다양한 텍스트 프롬프트를 처리할 수 있는 능력이 뛰어납니다.
	- **단점**: 각 샘플마다 최적화 과정을 거쳐야 하므로 계산 비용이 많이 들고 시간이 오래 걸릴 수 있습니다. 또한 강력한 3D prior가 없기 때문에, 의미 없는 local minima에 빠질 위험도 있습니다.

**본 논문의 접근법**


	![0](/assets/img/2024-06-29-[논문리뷰]-Point-E:-A-System-for-Generating-3D-Point-Clouds-from-Complex-Prompts-(Arxiv-2022).md/0.png)


논문에서는 두 가지 방법의 장점을 결합해 더욱 효율적인 text-to-3D 생성을 시도합니다. 연구진은 text-to-image 모델과 image-to-3D 모델을 함께 사용해, 복잡한 프롬프트에 대응하면서도 빠르게 3D 객체를 생성할 수 있는 방법을 제안했습니다.

- **Text-to-Image 모델**

	대규모 텍스트-이미지 쌍 데이터를 활용해 다양한 텍스트 프롬프트를 처리할 수 있는 능력을 갖추고 있습니다. 3D 렌더링에 최적화된 GLIDE 모델을 사용하여 이미지 샘플을 생성합니다.

- **Image-to-3D 모델**

	소규모의 이미지-3D 쌍 데이터를 학습한 디퓨전 모델을 사용하여 RGB 포인트 클라우드를 생성합니다. 이 과정에서 새롭게 설계된 transformer 기반 아키텍처를 도입하고, 생성된 포인트 클라우드에서 메쉬를 만들기 위해 회귀 기반 접근법을 적용합니다.


본 논문의 방법은 먼저 텍스트에서 이미지를 생성하고, 생성된 이미지를 조건으로 사용해 3D 객체를 생성하는 두 단계로 구성됩니다. 두 과정 모두 몇 초 내에 수행될 수 있으며, 기존 방식에서 요구되던 비용이 많이 드는 최적화 과정이 필요하지 않습니다.


**본 논문의 결과** 


이 방법을 통해 단순한 텍스트 프롬프트뿐만 아니라 복잡한 텍스트 프롬프트에 대한 컬러 3D 포인트 클라우드를 생성하는 데 성공했습니다. 3D 객체를 효율적으로 생성하는 이 시스템은 "Point E"라는 이름으로 불리며, 빠르고 효율적인 3D 생성의 새로운 가능성을 제시합니다.


# 2. Background 


**디퓨전 모델 개요**


디퓨전 모델은 데이터를 점진적으로 Gaussian 노이즈로 변형시켜 학습하는 생성 모델입니다. 이 과정은 데이터를 반복적으로 왜곡하여 점점 더 무작위에 가까운 상태로 바꾸고, 그 후 원래 상태로 복원하는 방식으로 작동합니다. 본 논문에서는 Ho et al. (2020)의 Gaussian 확산 설정을 따르며, 노이즈 추가 및 제거 과정을 통해 샘플을 생성합니다.


**노이즈 프로세스**


노이즈 프로세스는 각 시간 단계 $t$마다 신호에 Gaussian 노이즈를 더하는 방식으로 진행됩니다. 이 과정이 반복되면서, 마지막 단계에서는 샘플이 거의 정보를 포함하지 않게 됩니다. 이렇게 완전히 무작위화된 상태에서 모델은 점진적으로 정보를 복원하는 방향으로 학습됩니다.


**역 노이즈 프로세스**


노이즈 프로세스가 완료된 후, 랜덤하게 생성된 Gaussian 노이즈 샘플 $x_T$에서 시작하여 점차 노이즈를 제거해 나가면 최종적으로 잡음이 없는 샘플 $x_0$에 도달할 수 있습니다. 이 과정을 통해 모델은 완전한 샘플을 생성하게 됩니다.


**모델 학습**


모델은 시간 단계 $t$에서의 노이즈 상태 $q(x_{t-1}|x_t)$를 신경망 $p_\theta(x_{t-1}|x_t)$로 근사하는 방식으로 학습됩니다. Nichol & Dhariwal (2021)은 이때 평균뿐만 아니라 분산도 함께 예측함으로써 더 나은 성능을 얻을 수 있다고 제안했습니다.


**샘플링**


디퓨전 모델에서 샘플링은 미분 방정식의 관점에서 설명될 수 있습니다. 이로 인해 다양한 확률 미분 방정식(SDE) 및 보통 미분 방정식(ODE) 해석기를 활용해 샘플을 생성할 수 있습니다. 본 논문에서는 Karras et al. (2022)의 2차 ODE 해석기를 사용하여 빠르고 효율적인 샘플링을 수행합니다.


**가이드 전략**


Dhariwal & Nichol (2021)은 샘플의 충실도를 높이기 위해 **분류기 가이던스**(classifier guidance)를 도입했습니다. 이는 샘플링 과정에서 분류기를 활용해 더 구체적인 샘플을 생성할 수 있도록 돕습니다. 반면, Ho & Salimans (2021)은 **분류기 없는 가이던스**(classifier-free guidance)를 제안했습니다. 이 방법은 조건부 정보를 무작위로 삭제하여 모델의 학습 유연성을 높입니다. 본 논문에서는 학습 과정에서 드롭 확률 0.1을 사용하여 이 기술을 적용했습니다. 


# 3. Related Work

- 포인트 클라우드 생성 모델:
	- 포인트 클라우드 생성을 위해 다양한 접근 방식이 연구되고 있으며, 대표적으로 GAN, VAE, 플로우 모델, 디퓨전 모델 등이 사용되고 있습니다. 본 논문의 연구와 가장 유사한 PVD 모델은 단일 디퓨전 모델을 사용해 포인트 클라우드를 직접 생성합니다. 그러나 본 논문은 보다 간단한 트랜스포머 기반 아키텍처를 도입했으며, RGB 채널을 함께 생성하는 방식으로 차별점을 둡니다.
- 다른 3D 표현을 사용한 모델 생성:
	- 2D 이미지 데이터셋을 바탕으로 3D 인식 GAN을 학습하거나, 직접 3D 메쉬를 생성하는 연구들도 활발히 진행되고 있습니다. 이러한 연구들은 주로 새로운 뷰 합성 문제에 초점을 맞추지만, 전체 360도 뷰를 완전히 재구성하려는 시도는 많지 않습니다.
- 텍스트 조건부 3D 생성:
	- 몇몇 연구들은 텍스트-이미지 매칭을 목표로 하여 3D 표현을 최적화하는 접근 방식을 채택하고 있습니다. 이러한 접근법을 통해 다양한 복잡한 객체나 장면을 생성할 수 있지만, 최적화 절차는 매우 시간이 오래 걸리는 단점이 있습니다.
- 텍스트-3D 데이터 활용:
	- 텍스트-3D 쌍 데이터를 사용하여 텍스트 조건부 3D 모델을 생성하는 연구도 존재합니다. 그러나 대부분의 연구는 단순한 프롬프트나 좁은 범위의 객체 카테고리로 제한됩니다. 본 논문에서는 이러한 문제를 해결하기 위해 사전 학습된 텍스트-이미지 모델을 활용하는 방식을 채택했습니다.
- 이미지 기반 3D 재구성:
	- 단일 또는 소수의 이미지에서 3D 모델을 재구성하는 회귀 기반 및 생성 접근 방식이 있습니다. 이들 접근 방식은 불충분한 데이터 문제를 해결하려는 노력이 포함하면서도 유망한 결과를 보여주고 있습니다.

# 4. Method


본 논문에서는 텍스트를 조건으로 받아 단일 생성 모델로 포인트 클라우드를 직접 생성하는 대신, 이를 세 단계로 나누어 보다 효율적으로 생성 과정을 진행합니다.

1. 텍스트 캡션을 조건으로 합성 뷰 생성

	첫 번째 단계에서는 텍스트 캡션을 입력으로 받아 ‘합성 뷰’를 생성합니다. 이 과정에는 GLIDE 모델을 사용하며, 3D 모델의 렌더링에 맞게 파인튜닝된 버전을 적용합니다.


	→ 자세한 내용은 _4.2_에서 다룹니다.

2. 합성 뷰를 조건으로 대략적인 포인트 클라우드 생성

	두 번째 단계에서는 생성된 합성 뷰를 조건으로 하여 대략적인 포인트 클라우드(1024개의 포인트)를 생성합니다. 이 과정에서는 조건부 순열 불변 디퓨전 모델을 사용해 포인트 클라우드를 생성합니다.


	→ 자세한 내용은 _4.3_에서 다룹니다.

3. 저해상도 포인트 클라우드와 합성 뷰를 조건으로 고해상도 포인트 클라우드 생성

	마지막 단계에서는 저해상도 포인트 클라우드(1024 포인트)와 합성 뷰를 조건으로 하여 고해상도 포인트 클라우드(4096 포인트)를 생성합니다. 이때는 두 번째 단계에서 사용된 디퓨전 모델과 유사하지만, 저해상도 포인트 클라우드를 조건으로 하는 더 작은 모델을 활용합니다.


	→ 자세한 내용은 _4.4_에서 다룹니다.


본 논문에서는 수백만 개의 3D 모델과 그에 관련된 메타데이터로 구성된 대규모 데이터셋을 사용해 모델을 훈련시킵니다. 이 데이터셋은 렌더링된 뷰, 텍스트 설명, 그리고 각 점에 대한 RGB 색상을 포함하는 3D 포인트 클라우드로 처리하여 다양한 정보와 조건을 모델에 제공하는 데 활용됩니다.


## 4-1. Dataset


본 논문에서는 수백만 개의 3D 모델을 활용하여 데이터셋을 구축하고자 하였는데 데이터 형식과 품질이 매우 다양했습니다. 따라서, 더 높은 품질의 데이터를 확보하기 위해 여러 후처리 단계를 적용해야 했습니다. 이 과정은 다음과 같이 진행되었습니다.

1. Blender를 사용한 데이터 형식 통일화

	먼저, Blender 프로그램을 사용하여 모든 데이터를 하나의 일반적인 형식인 RGBAD 이미지로 변환했습니다. Blender는 다양한 3D 형식을 지원하며, 최적화된 렌더링 엔진을 제공하는 프로그램입니다. RGBAD 이미지는 RGB 이미지에 깊이(Depth)와 알파(Alpha) 채널이 추가된 형식입니다.

	- 각 3D 모델은 20개의 랜덤한 카메라 각도에서 경계 상자(bounding cube)로 정규화되었으며, 표준 조명 설정을 구성한 후 Blender의 실시간 렌더링 엔진을 사용해 RGBAD 이미지를 내보냈습니다.
2. 렌더링을 통한 색상 포인트 클라우드 변환

	이후 각 객체는 렌더링된 RGBAD 이미지를 바탕으로 색상이 있는 포인트 클라우드로 변환되었습니다.

	- 각 RGBAD 이미지의 각 픽셀에 대한 점을 계산하여 객체에 대한 밀집된 포인트 클라우드를 생성했습니다.
	- 포인트 클라우드는 고르게 분포되지 않기 때문에, 가장 먼 점 샘플링을 사용하여 4K 점의 균일한 포인트 클라우드를 생성했습니다.
	- 이러한 렌더링 기반 포인트 클라우드 구성 방식은, 3D 메쉬에서 직접 점을 샘플링할 때 발생할 수 있는 문제(모델 내부 점을 샘플링하는 문제나 이상한 파일 형식의 3D 모델로 인한 문제)를 피할 수 있게 해주었습니다.
3. 저품질 모델 제거

	마지막으로, 저품질의 3D 모델을 제거하기 위해 다양한 휴리스틱을 적용했습니다.

	- 각 포인트 클라우드의 SVD(특이값 분해)를 계산하여, 가장 작은 특이값이 일정 임계값(threshold) 이상인 경우에만 해당 모델을 유지함으로써 평평한 객체를 제거했습니다.
	- 이후, CLIP 특성을 기반으로 데이터셋을 클러스터링하였습니다. 일부 클러스터는 많은 저품질 모델을 포함한 반면, 다른 클러스터는 더 다양하고 해석 가능한 것으로 나타났습니다.
	- 최종적으로 클러스터를 여러 품질의 버킷으로 나누고, 결과 버킷의 가중치 혼합을 사용하여 최종 데이터셋을 구성했습니다.

이러한 후처리 과정을 통해 다양한 형식과 품질의 3D 모델을 정리하고, 높은 품질을 유지할 수 있는 포인트 클라우드를 생성할 수 있었습니다.


## 4.2 View Synthesis GLIDE Model


본 연구에서는 '텍스트 캡션'을 조건으로 받아 '합성 뷰'를 생성하는 모델을 제안합니다. 이 모델은 4.3에서 설명할 포인트 클라우드 생성 모델과 연동되며, 모든 포인트 클라우드 모델은 동일한 렌더러와 조명 설정을 기반으로 생성된 데이터셋의 렌더링된 뷰를 조건으로 받습니다. 따라서, 본 파트에서는 데이터셋 분포와 일치하는 3D 렌더를 명시적으로 생성하는 것을 목표로 합니다.


이를 위해 **GLIDE 모델**을 원래 데이터셋과 연구진이 구축한 3D 렌더링 데이터셋을 혼합하여 파인튜닝하였습니다. 이때, 몇 가지 중요한 요소들을 고려하였습니다:

1. 데이터셋 샘플링 비율 조정

	연구진이 수집한 3D 렌더링 데이터셋은 원래 GLIDE 학습 데이터셋에 비해 크기가 작았기 때문에, 3D 렌더링 데이터셋에서 샘플링되는 비율을 5%로 설정하고, 나머지 95%는 원래 GLIDE 학습 데이터셋을 사용하여 학습이 진행되었습니다. 이로써, 모델이 원래 학습된 이미지 데이터셋을 기반으로 하되, 3D 렌더링 특성도 학습할 수 있게 했습니다.

2. 반복 횟수

	학습 반복(iteration) 횟수는 100,000번으로 설정하여, 모델이 3D 데이터셋을 여러 번 학습하도록 했습니다. 단, 동일한 렌더링된 시점을 두 번 사용하는 일은 없도록 하여, 모델이 새로운 데이터로 충분히 학습할 수 있게 조정했습니다.

3. 특별한 토큰 추가

	테스트 시점에서는 항상 데이터셋 분포 내에서 렌더를 샘플링할 수 있도록, 모든 3D 렌더의 텍스트 프롬프트에 특별한 토큰을 추가했습니다. 이를 통해, 모델이 테스트 시간에 이 토큰을 기반으로 샘플링을 수행하며, 원하는 결과를 생성할 수 있도록 했습니다.


결과적으로, 본 모델은 텍스트를 조건으로 받아 3D 렌더링된 합성 뷰를 효과적으로 생성할 수 있으며, 파인튜닝된 GLIDE 모델 덕분에 데이터셋과 일치하는 고품질의 뷰를 생성하는 데 성공했습니다.


## 4.3  **Point Cloud Diffusion**


이 연구에서는 합성 뷰를 조건으로 하여 대략적인 포인트 클라우드를 생성하는 모델을 제안합니다. 포인트 클라우드 생성을 위해 [3D Shape Generation and Completion through Point-Voxel Diffusion](https://arxiv.org/abs/2104.03670)에서 제시한 프레임워크를 확장하여, 각 포인트에 RGB 색상을 포함시켰습니다.


주요 특징은 다음과 같습니다:

1. 포인트 클라우드 표현

	포인트 클라우드는 K×6 형태의 텐서로 나타내며, 여기서 K는 포인트 수를 나타내고, 내부 차원은 (x, y, z) 좌표와 (R, G, B) 색상으로 구성됩니다. 모든 좌표와 색상 값은 [-1, 1] 범위로 정규화됩니다. 이러한 텐서는 랜덤한 노이즈에서 시작하여, 점진적으로 디노이징하는 과정을 통해 포인트 클라우드를 직접 생성합니다.

2. 모델 구조

	기존 연구들에서는 3D 전용 구조를 사용하였으나, 본 논문에서는 **트랜스포머 기반 모델**을 사용합니다. 이 모델은 이미지, 타임스텝 t, 그리고 노이즈가 있는 포인트 클라우드 $x_t$를 조건으로 받아 $\epsilon$과 Σ를 예측합니다.

- 모델 구조

	![1](/assets/img/2024-06-29-[논문리뷰]-Point-E:-A-System-for-Generating-3D-Point-Clouds-from-Complex-Prompts-(Arxiv-2022).md/1.png)

	- 포인트 클라우드 입력

		각 포인트는 출력 차원이 D인 선형 레이어(linear layer)를 통과해 K×D 크기의 입력 텐서를 형성합니다. 여기에 타임스텝 $t$는 작은 MLP에 넣어 또 다른 D차원 벡터를 생성하고, 이를 컨텍스트 앞에 추가합니다.

	- 이미지 임베딩

		이미지를 조건으로 입력받기 위해 **사전 학습된 CLIP ViT-L/14 모델**에 이미지를 입력합니다. 이 모델의 마지막 레이어에서 얻은 임베딩(shape: 256 x D’)을 선형 투사(linearly project)하여, 256 x D 크기의 또 다른 텐서를 생성합니다. 이 텐서는 트랜스포머 컨텍스트 앞에 추가됩니다. 이 방법은 단일 CLIP 이미지나 텍스트 임베딩을 사용하는 것보다 우수한 성능을 보였습니다.

	- 최종 컨텍스트

		최종 입력 컨텍스트는 (K+257) x D크기의 텐서가 됩니다. 이 텐서에서 길이 K의 최종 출력 시퀀스를 얻고, 이를 다시 프로젝션하여 입력된 K개의 포인트에 대한 ϵ과 Σ 예측값을 얻습니다.


> 💡 정리  
> - 입력 컨텍스트 구성:  
>   
> - 트랜스포머 모델의 출력: (K+257)개의 토큰 (각 토큰의 차원은 D)  
>   
> - 최종 출력 시퀀스 선택: 최종 K개의 토큰을 가져온다.  
>   
> - ε와 Σ 예측**:** 최종 K개의 토큰을 ε와 Σ 예측을 위한 입력 포인트로 사용한다.  
>   
> - 예측된 ε와 Σ을 통해 노이즈를 제거하여 포인트 클라우드를 복원

	- 입력 컨텍스트 구성:
		- 포인트 클라우드의 각 점: K×D
		- CLIP 이미지 임베딩: 256×D
		- 타임스텝 임베딩: 1×D

		→ 최종 입력 컨텍스트: (K+257)×D

	- 트랜스포머 모델의 출력: (K+257)개의 토큰 (각 토큰의 차원은 D)
	- 최종 출력 시퀀스 선택: 최종 K개의 토큰을 가져온다.
	- ε와 Σ 예측**:** 최종 K개의 토큰을 ε와 Σ 예측을 위한 입력 포인트로 사용한다.
	- 예측된 ε와 Σ을 통해 노이즈를 제거하여 포인트 클라우드를 복원
- 이 모델에서는 positional encoding을 사용하지 않는다. 따라서 모델 자체는  입력 포인트 클라우드에 대해 순열 분별(permutation-invariant)하다.

## 4.4 **Point Cloud Upsampler**

- 이미지 디퓨전 모델에서의 계층 구조
	- 이미지 디퓨전 모델에서 최상의 품질을 달성하는 방법 중 하나는 계층 구조를 사용하는 방식입니다. 이 방식은 저해상도의 기본 모델이 출력을 생성하고, 이를 다른 모델이 업샘플하는 구조로 이루어집니다.

	→ 포인트 클라우드 생성에 이 접근 방식을 사용

- 포인트 클라우드 생성에서의 계층 구조
	- 포인트 클라우드 생성에서도 계층 구조 접근 방식을 도입하여, 먼저 1K 포인트를 생성하는 큰 베이스 모델을 사용하고, 이후 작은 업샘플링 모델을 통해 이를 4K 포인트로 업샘플링합니다.
	- 모델 크기가 동일할 경우, 4K 포인트를 생성하는 데는 1K 포인트를 생성할 때보다 네 배 더 많은 연산이 필요합니다.
- 업샘플러
	- 업샘플러는 베이스 모델과 동일한 아키텍처를 사용하지만, 저해상도 포인트 클라우드를 입력받기 위한 추가적인 컨디셔닝 토큰이 포함됩니다.
	- 이 모델은 1K 포인트를 조건으로 받아 추가로 3K 포인트를 생성하고, 이를 저해상도 포인트 클라우드에 추가합니다.
	- 저해상도 포인트는 $x_t$에 사용된 레이어가 아닌 별도의 선형 임베딩 레이어를 통해 전달됩니다. 이를 통해, 모델이 positional encoding 없이도 조건부 정보와 새로운 포인트를 명확히 구별할 수 있습니다.

## 4.5 Producing Meshes


포인트 클라우드를 렌더링 기반으로 평가할 때, 생성된 포인트 클라우드를 직접 렌더링하지 않고, 이를 텍스처가 입혀진 메쉬로 변환한 후 Blender를 사용해 메쉬를 렌더링하는 방법을 채택하였습니다. 포인트 클라우드에서 메쉬를 생성하는 과정은 때때로 도전적이며, 본 논문에서 생성한 포인트 클라우드는 균열, 이상치, 또는 기타 노이즈를 포함하고 있어 작업이 더욱 어려웠습니다.


기존의 사전 학습된 SAP 모델을 사용하여 포인트 클라우드에서 메쉬를 생성해 보았으나, 이 모델은 포인트 클라우드의 중요한 세부 사항이나 큰 부분을 잃어버리는 경우가 발생했습니다. 이를 해결하기 위해 본 논문에서는 회귀 기반 모델을 사용하여 signed distance field를 예측하고, marching cubes 알고리즘을 적용하여 메쉬를 추출하였습니다. 이후, 추출된 메쉬의 각 버텍스에는 원래 포인트 클라우드에서 가장 가까운 점의 색상을 할당하여 텍스처를 입혔습니다.


> 💡 - 렌더링 과정 요약

	- 렌더링 과정 요약
		1. 포인트 클라우드에서 SDF 예측: 회귀 기반 모델을 사용하여 포인트 클라우드로부터 객체의 SDF를 예측한다.
		2. 메쉬 생성: 예측된 SDF를 기반으로 merching cube 알고리즘을 적용하여 메쉬를 생성합니다.
		3. 색상 할당: 생성된 메쉬의 각 버텍스에 원래 포인트 클라우드의 색상을 할당하여 텍스처가 입혀진 메쉬를 만든다.
		4. Blender를 통한 렌더링: 최종적으로 텍스처가 입혀진 메쉬를 Blender를 사용하여 렌더링한다.

# 5. Results

- 평가 지표: CLIP R-Precision, P-IS, P-FID
	- CLIP R-Precision
		- 특정 객체를 기준으로 하여 모델이 텍스트 설명과 얼마나 잘 일치하는지를 평가하는 지표입니다.
		- 계산하는 과정은 다음과 같습니다:
			1. 생성된 이미지와 텍스트 프롬프트를 기반으로 CLIP 모델을 사용하여 각 이미지의 텍스트 임베딩을 계산합니다.
			2. CLIP 모델에서 계산된 텍스트 임베딩과 이미지 임베딩 간의 유사도를 계산합니다.
			3. 유사도가 가장 높은 상위 R개의 이미지 중 실제로 맞는 이미지의 비율을 계산합니다.
	- P-IS, P-FID
		- 포인트 클라우드의 Inception Score와 FID를 평가하기 위해 본 논문에서 도입한 지표입니다.
		- 수정된 PointNet++ 모델을 사용하여 포인트 클라우드에서 특징을 추출하고 클래스 확률을 예측합니다.

## 5.1 **Model Scaling and Ablations**


저자들은 다음과 같은 베이스 모델에 대하여 학습 중에 생성한 샘플들로 평가하였습니다.

- 40M (uncond.): 어떠한 조건 정보도 없는 작은 모델
- 40M (text vec.): 텍스트 캡션에만 의존하는 작은 모델 (이미지 사용 x), 파인튜닝된 GLIDE 모델 활용 x
- 40M (image vec.): 렌더링된 이미지의 CLIP 이미지 임베딩에 의존하는 작은 모델, 단일 CLIP 임베딩 사용
- 40M: CLIP 잠재 그리드(latent grid)를 통한 전체 이미지 조건을 사용하는 작은 모델
- 300M: CLIP 잠재 그리드를 통한 전체 이미지 조건을 사용하는 중간 모델
- 1B: CLIP 잠재 그리드를 통한 전체 이미지 조건을 사용하는 큰 모델

평가 결과는 아래 그래프와 같습니다.


![2](/assets/img/2024-06-29-[논문리뷰]-Point-E:-A-System-for-Generating-3D-Point-Clouds-from-Complex-Prompts-(Arxiv-2022).md/2.png)

- 결과
	- **텍스트 조건만 사용하고 텍스트에서 이미지로의 단계가 없는 경우**에는 CLIP R-Precision이 매우 낮게 나타나는 것을 발견했습니다.
	- **이미지를 조건으로 사용할 때** 단일 CLIP 임베딩보다 임베딩 그리드를 사용하는 것이 성능이 더 우수하다는 것을 발견했습니다. 이는 조건 이미지에 대해 더 많은 (공간적인) 정보를 제공하는 것이 포인트 클라우드 모델에 유리하다는 것을 시사합니다.
	- **모델의 스케일을 증가시키면** P-FID의 수렴 속도가 향상되고 최종 CLIP R-Precision이 증가하는 것을 발견했습니다.

## 5.2 Qualitative Results

- 포인트 클라우드 생성 결과

	![3](/assets/img/2024-06-29-[논문리뷰]-Point-E:-A-System-for-Generating-3D-Point-Clouds-from-Complex-Prompts-(Arxiv-2022).md/3.png)


Point·E 모델은 복잡한 프롬프트에 대해서도 일관된 고품질의 3D 형상을 생성할 수 있는 능력을 보여주었습니다. 그러나 때때로 포인트 클라우드 디퓨전 모델이 조건화된 이미지를 제대로 이해하지 못하거나 예측할 수 없는 경우가 발생하기도 합니다. 이러한 문제는 주로 두 가지 원인 중 하나로 인해 발생합니다.

1. 모델이 이미지에 나타난 객체의 모양을 잘못 해석하는 경우
2. 모델이 이미지에서 가려진 형상의 일부를 잘못 추론하는 경우

	![4](/assets/img/2024-06-29-[논문리뷰]-Point-E:-A-System-for-Generating-3D-Point-Clouds-from-Complex-Prompts-(Arxiv-2022).md/4.png)_잘못 추론한 예시_


## **5.3 Comparison to Other Methods**


CLIP-R-Precision 지표를 사용하여 Point·E를 다른 3D 생성 기술과 비교한 결과는 다음과 같습니다.


	![5](/assets/img/2024-06-29-[논문리뷰]-Point-E:-A-System-for-Generating-3D-Point-Clouds-from-Complex-Prompts-(Arxiv-2022).md/5.png)


Point·E는 최신 기술인 DreamFusion보다 성능이 다소 낮은 것으로 나타났습니다. 그러나 이 불일치를 설명할 수 있는 두 가지 미묘한 점이 있습니다. 첫째, DreamFusion과 같은 멀티뷰 최적화 기반 방법과 달리, Point·E는 텍스트 프롬프트와 일치하도록 모든 뷰를 명시적으로 최적화하지 않습니다. 이로 인해 특정 객체가 모든 각도에서 쉽게 식별되지 않을 수 있으며, CLIP R-Precision이 낮아질 수 있습니다. 둘째, 본 논문에서 제안하는 방법은 렌더링 전에 포인트 클라우드를 전처리해야 하는데, 포인트 클라우드를 메쉬로 변환하는 과정에서 원본 포인트 클라우드의 정보가 손실될 수 있습니다.


Point·E는 최신 기술보다 이 평가에서 성능이 좋지 않지만, 짧은 시간 내에 샘플을 생성할 수 있습니다. 이를 통해 보다 실용적인 응용 프로그램을 개발하거나 많은 개체를 샘플링한 후, 최상의 개체를 휴리스틱을 통해 선택하여 고품질의 3D 개체를 찾는 데 유용할 수 있습니다.


# **6. Limitations and Future Work**


현재 모델은 합성 렌더링을 필요로 합니다. 향후에는 실제 세계 이미지를 조건으로 하는 3D 생성기를 훈련시킴으로써 이 문제를 해결할 수 있을 것으로 보입니다. 현재 모델은 색상이 있는 3D 형태를 생성하지만, 이 과정은 비교적 낮은 해상도의 3D 형식인 포인트 클라우드를 사용하여 이루어집니다. 따라서 형상이나 질감의 세부 사항을 충분히 캡처하지 못합니다. 이 문제는 메쉬나 NeRF와 같은 고해상도 3D 표현을 생성하도록 모델을 확장하면 해결할 수 있을 것입니다.


또한, 최적화 기반 기술(optimization-based techniques)을 초기화하여 초기 수렴 속도를 높이는 데 사용할 수 있습니다. 이 모델은 DALL·E 2 시스템과 비슷한 많은 제한 사항을 공유할 것으로 예상되며, 이는 데이터셋에서 유래된 여러 편향을 포함할 수 있다는 점의 제한 사항이 있을 수 있습니다. 마지막으로, 모델이 생성한 3D 모델이 실제로 물리적으로 제작될 때, 이 제품이 위험할 수 있는 물체의 청사진을 생성할 수도 있습니다.


	![6](/assets/img/2024-06-29-[논문리뷰]-Point-E:-A-System-for-Generating-3D-Point-Clouds-from-Complex-Prompts-(Arxiv-2022).md/6.png)


# 7. Conclusion


Point·E는 합성된 뷰를 생성하고 이를 기반으로 조건화된 색상 포인트 클라우드를 생성하는 텍스트 조건 합성 시스템입니다. 연구 결과, Point·E는 텍스트 프롬프트에 의해 조건화된 다양한 복잡한 3D 형상을 효율적으로 생성할 수 있는 능력을 갖추고 있음을 발견했습니다. 본 논문에서는 이 방식이 텍스트에서 3D로의 합성 분야에서 추가적인 연구의 시작점으로 기여할 수 있기를 희망한다는 끝으로 논문을 마쳤습니다. 

